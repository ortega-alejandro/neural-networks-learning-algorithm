# Stochastic Gradient Descent for Neural Networks

Final project for Math 465/CS 445 _High Dimensional Data Analysis_ taught by Dr. Paul Bendich @ Duke University, Fall 2017. This project consisted of a 20+ minute lecture describing the backpropagation via gradient descent algorithm that neural networks use to learn, and the improvements that stochastic gradient descent provide over standard (or batch) gradient descent in high dimensions. For an example of training a neural network in TensorFlow, visit this __[Google Colab Notebook](https://colab.research.google.com/notebooks/mlcc/intro_to_neural_nets.ipynb)__

### Course Grade Earned: A-

### Project Members: Alejandro Ortega, Peter Mikhael 
